"""
    Name: Xiuyi Feng
    Email: xiuyi.feng15@myhunter.cuny.edu
    Resources:
                https://learningds.org/ch/16/ms_overfitting.html
                https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html
                https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html
"""

import pickle
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression,LassoCV, RidgeCV
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
def import_data(csv_file):
    """
        The data in the file is read into a DataFrame, and a new column, units,
        that indexes the lines of the files and represents the number of units.
        It can be generated by looping through the dataset or using the index as a column.
        The resulting DataFrame is returned.
    """
    dff=pd.read_csv(csv_file)
    dff['units']=dff.index.to_series()
    return dff

def split_data(dff, y_col_name, test_size = 0.25, random_state = 21):
    """
        Returns the data split into 4 subsets, corresponding to those returned
        by train_test_split: x_train, x_test, y_train, and y_test. where units is
        the "x" column and the input parameter, y_col_name is the "y" column.
    """
    xes = dff['units']
    yes = dff[y_col_name]
    x_train, x_test, y_train, y_test = train_test_split(xes, yes,
    random_state = random_state, test_size = test_size)
    return x_train, x_test, y_train, y_test


def fit_poly(xes, yes, epsilon=100, verbose=False):
    """
        It returns the smallest integer degree >= 1 for which the model yields a
        MSE of less than the specified epsilon and the coefficients as a vector
        for df["units"] and df[y_col]. If it does not find a model with an error
        less than epsilon by degree 5, returns None. When fitting the linear regression model,
        the fit_intercept=False.
    """
    for deg in range(1, 5):
        poly = PolynomialFeatures(degree = deg)
        poly_features = poly.fit_transform(xes)
        model_deg = LinearRegression(fit_intercept=False)
        model_deg.fit(poly_features,yes)
        y_pred =  model_deg.predict(poly_features)
        error = mean_squared_error(yes, y_pred)
        if verbose:
            print(f'MSE cost for deg {deg} poly model: {error:.3f}')
        if error < epsilon:
            return deg
    return None

def fit_model(xes, yes, poly_deg=2, reg = "lasso"):
    """
        This function fits a model with polynomial features using Lasso or
        Ridge regression with cross validation:
        Apply PolynomialFeatures to the xes with degree equal to poly_deg.
        If reg equals ridge, use RidgeCV to instantiate and fit a model to the polynomial
        features and yes. Otherwise, use LassoCV to to instantiate and fit the model.
        Returns the model as serialized object (i.e. a pickled object).
    """
    poly= PolynomialFeatures(degree = poly_deg,include_bias=False)
    poly_features = poly.fit_transform(xes)
    if reg == "ridge":
        reg_cv = RidgeCV(fit_intercept=False)
    elif reg == "lasso":
        reg_cv = LassoCV(fit_intercept=False)
    reg_cv.fit(poly_features, yes)
    reg_pickle = pickle.dump(reg_cv)
    return reg_pickle

def predict_using_trained_model(mod_pkl, poly_xes, yes):
    """
        Computes and returns the mean squared error and r2 score between the
        values predicted by the model (mod on x) and the actual values (y).
        Note that sklearn.metrics contains two functions that may be of use:
        mean_squared_error and r2_score
    """
    #with open(mod_pkl, 'rb') as model_f:
    lr1 = pickle.loads(mod_pkl)
    prediction = lr1.predict(poly_xes)
    mse = mean_squared_error(yes, prediction)
    r2_s = r2_score(yes, prediction)
    return mse, r2_s
